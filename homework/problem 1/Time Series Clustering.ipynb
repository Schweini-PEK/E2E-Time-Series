{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LB Keogh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the _LB Keogh_ lower bound of dynamic time warping\n",
    "\n",
    "$$\n",
    "\\begin{split}LB_{Keogh}(Q, (L, U)) = \\sqrt{\\sum_{i=1}^n\n",
    "\\begin{cases}\n",
    "      (q_i - u_i)^2 & \\text{if $q_i > u_i$}\\\\\n",
    "      (q_i - l_i)^2 & \\text{if $q_i < l_i$}\\\\\n",
    "      0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "}\\end{split}\n",
    "$$\n",
    "\n",
    "where $U_i$ and $L_i$ are upper and lower bounds for time series $Q$ which are defined as $U_i=max(q_{i-r}:q_{i+r})$ and $L_i=min(q_{i-r}:q_{i+r})$ for a reach $r$. It can be implemented with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LB_Keogh(s1,s2,r):\n",
    "    LB_sum=0\n",
    "    for ind,i in enumerate(s1):\n",
    "        \n",
    "        lower_bound=min(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n",
    "        upper_bound=max(s2[(ind-r if ind-r>=0 else 0):(ind+r)])\n",
    "        \n",
    "        if i>upper_bound:\n",
    "            LB_sum=LB_sum+(i-upper_bound)**2\n",
    "        elif i<lower_bound:\n",
    "            LB_sum=LB_sum+(i-lower_bound)**2\n",
    "    \n",
    "    return np.sqrt(LB_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using datasets from A. Bagnall, J. Lines, W. Vickers and E. Keogh, The UEA & UCR Time Series Classification Repository, www.timeseriesclassification.com\n",
    "\n",
    "In this example, time series are preprocessed using TimeSeriesScalerMeanVariance. This scaler is such that each output time series has zero mean and unit variance. The assumption here is that the range of a given time series is uninformative and one only wants to compare shapes in an amplitude-invariant manner (when time series are multivariate, this also rescales all modalities such that there will not be a single modality responsible for a large part of the variance). This means that one cannot scale barycenters back to data range because each time series is scaled independently and there is hence no such thing as an overall data range.\n",
    "\n",
    "[1] F. Petitjean, A. Ketterlin & P. Gancarski. A global averaging method for dynamic time warping, with applications to clustering. Pattern Recognition, Elsevier, 2011, Vol. 44, Num. 3, pp. 678-693 [2] M. Cuturi, M. Blondel “Soft-DTW: a Differentiable Loss Function for Time-Series,” ICML 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tslearn\n",
    "from tslearn.datasets import CachedDatasets\n",
    "\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.datasets import CachedDatasets\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, \\\n",
    "    TimeSeriesResampler\n",
    "\n",
    "np.random.seed(42)\n",
    "X_train, y_train, X_test, y_test = CachedDatasets().load_dataset(\"Trace\")\n",
    "X_train = X_train[y_train < 4]  # Keep first 3 classes\n",
    "y_train = y_train[y_train < 4]\n",
    "y_train = y_train-1\n",
    "\n",
    "#np.random.shuffle(X_train)\n",
    "indices = np.arange(X_train.shape[0])\n",
    "indices = np.random.shuffle(indices)\n",
    "X_train = X_train[indices][0]\n",
    "y_train = y_train[indices][0]\n",
    "\n",
    "# Keep only 50 time series\n",
    "X_train = TimeSeriesScalerMeanVariance().fit_transform(X_train[:50])\n",
    "y_train = y_train[:50]\n",
    "# Make time series shorter\n",
    "X_train = TimeSeriesResampler(sz=40).fit_transform(X_train)\n",
    "sz = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visulize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,3))\n",
    "for yi in range(3):\n",
    "    plt.subplot(1, 3, yi+1)\n",
    "    for xx in X_train[y_train == yi]:\n",
    "            plt.plot(xx.ravel(), \"k-\", alpha=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we only use some data from tslearn package and from now on all parts should not be relying on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally a K-Means Clustering takes the mean value of each cluster as its centroid in each iteration. A wrong example of time series is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clust(X,num_clust,num_iter):\n",
    "    \n",
    "    # initial random centroids   \n",
    "    centroids = random.sample(list(X),num_clust)\n",
    "\n",
    "    for n in tqdm(range(num_iter)):\n",
    "        y_pred = []\n",
    "        assignments={}\n",
    "        #assign data points to clusters\n",
    "        for ind,i in enumerate(X_train):\n",
    "            min_dist=float('inf')\n",
    "            closest_clust=None\n",
    "            for cluster_id,centroid in enumerate(centroids):\n",
    "                if LB_Keogh(i,centroid,10)<min_dist:\n",
    "                    cur_dist=LB_Keogh(i,centroid,10)\n",
    "                    if cur_dist<min_dist:\n",
    "                        min_dist=cur_dist\n",
    "                        closest_clust=cluster_id\n",
    "                        \n",
    "            if closest_clust in assignments:\n",
    "                assignments[closest_clust].append(ind)\n",
    "            else:\n",
    "                assignments[closest_clust]=[]\n",
    "            y_pred.append(closest_clust)\n",
    "            \n",
    "        #recalculate centroids of clusters using simple average of time series\n",
    "        for key in assignments:\n",
    "            clust_sum=0\n",
    "            for k in assignments[key]:\n",
    "                clust_sum=clust_sum+X[k]\n",
    "            \n",
    "            if len(assignments[key])>0:\n",
    "                centroids[key]=clust_sum/len(assignments[key])\n",
    "                \n",
    "    return np.array(y_pred), centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, centroids=k_means_clust(X_train,3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,3))\n",
    "\n",
    "for yi in range(3):\n",
    "    plt.subplot(1, 3, yi+1)\n",
    "    for xx in X_train[y_pred == yi]:\n",
    "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
    "    plt.plot(centroids[yi].ravel(), \"r-\")\n",
    "    plt.xlim(0, sz)\n",
    "    plt.ylim(-4, 4)\n",
    "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
    "             transform=plt.gca().transAxes)\n",
    "    if yi == 1:\n",
    "        plt.title(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the clustering accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "for i in [0,1,2]:\n",
    "    temp = sum(np.mod(y_pred+i,3) == y_train)/len(y_train)\n",
    "    if accuracy < temp:\n",
    "        accuracy = temp\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the result meet your expectation? Why is it so? Type your answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recall that a centroid of a cluster of points is a point that minimize the mean square of distances to other points in its cluster, a centroid of a time series should share a similar property. In previous parts, we have seen the appropriate tool to describe distance between 2 time series. With this in mind, finish the following part.\n",
    "\n",
    "Hint: such time series centroid can be one member of its cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lb_k_means_clust(X,num_clust,num_iter):\n",
    "    \n",
    "    # initial random centroids   \n",
    "    centroids = random.sample(list(X),num_clust)\n",
    "\n",
    "    for n in tqdm(range(num_iter)):\n",
    "        y_pred = []\n",
    "        assignments={}\n",
    "        #assign data points to clusters\n",
    "        for ind,i in enumerate(X_train):\n",
    "            min_dist=float('inf')\n",
    "            closest_clust=None\n",
    "            for cluster_id,centroid in enumerate(centroids):\n",
    "                if LB_Keogh(i,centroid,10)<min_dist:\n",
    "                    cur_dist=LB_Keogh(i,centroid,10)\n",
    "                    if cur_dist<min_dist:\n",
    "                        min_dist=cur_dist\n",
    "                        closest_clust=cluster_id\n",
    "                        \n",
    "            if closest_clust in assignments:\n",
    "                assignments[closest_clust].append(ind)\n",
    "            else:\n",
    "                assignments[closest_clust]=[]\n",
    "            y_pred.append(closest_clust)\n",
    "            \n",
    "            #recalculate centroids of clusters using LB Keogh\n",
    "            ### BEGIN CODE ###\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ### END CODE ###\n",
    "            \n",
    "    return np.array(y_pred), centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred, centroids=lb_k_means_clust(X_train,3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,3))\n",
    "\n",
    "for yi in range(3):\n",
    "    plt.subplot(1, 3, yi+1)\n",
    "    for xx in X_train[y_pred == yi]:\n",
    "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
    "    plt.plot(centroids[yi].ravel(), \"r-\")\n",
    "    plt.xlim(0, sz)\n",
    "    plt.ylim(-4, 4)\n",
    "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
    "             transform=plt.gca().transAxes)\n",
    "    if yi == 1:\n",
    "        plt.title(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the clustering accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "for i in [0,1,2]:\n",
    "    temp = sum(np.mod(y_pred+i,3) == y_train)/len(y_train)\n",
    "    if accuracy < temp:\n",
    "        accuracy = temp\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
